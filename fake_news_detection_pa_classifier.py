# -*- coding: utf-8 -*-
"""Fake_news_detection_PA_Classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1h9s_vFn1GIwu2B3bexg35lBQvM2m_NHV
"""

# Import packages and Libraries
import os
import numpy as np
import pandas as pd

# ML toolkits
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score
from sklearn.utils.extmath import density
from sklearn.pipeline import make_pipeline

# Data visualization
import matplotlib.pyplot as plt
import seaborn as sns

fake = pd.read_csv('/content/drive/MyDrive/Colab Datasets/Fake_news_dataset.csv')
true = pd.read_csv('/content/drive/MyDrive/Colab Datasets/True_news_dataset.csv')
print (fake)
print(true)

display(fake.head())
display(true.head())

"""Now I shall perform the below on our dataset
* Data Exploration
* Data cleaning & Preparation
* Feature Extraction
* Model tesing & training
"""

display(fake.info())
print('\n')
display(true.info())

display(fake.subject.value_counts())
print('\n')
display(true.subject.value_counts())

"""### Data Cleaning & Preparation

It looks like the subject column is too informative - there are no overlapping "subjects" between fake and true news articles. Since we want to build a model that can differentiate fake and true news based on its content we will drop this column.

However, the best way to train the model on both fake and true news data will be to concatenate two kinds and shuffle them. We will add the labels to start with.
"""

fake['label'] = 'fake'
true['label'] = 'real'

data = pd.concat([fake, true], axis=0)
data = data.sample(frac=1).reset_index(drop=True)
data.drop('subject', axis=1)

# Split the dataset to test and train
X_train, X_test, y_train, y_test = train_test_split(data['text'], data['label'], test_size=0.25)
display(X_train.head())
print('\n')
display(y_train.head())

print("\nThere are {} documents in the training data.".format(len(X_train)))

"""### Feature Extraction

Before getting into feature extraction, let's add some explanation to the method that was used.

Few common methods of extracting numerical features from text are tokenizing, counting occurrence and tf-idf term weighting. From thiswe will choose tf-idf term weighting as the feature to extract from these text.

- Term frequency
"tf" refers to the term frequency which indicates how often the terms can be found in documents. Tf's alone are often not sufficient as features as there are many commonly-used words such as "is", "are", "the", etc. which do not carry much information about the document hence, we do not want to take these terms as informative terms compared to others. These uninformative terms are actually referred to as stop words, and are often cleaned out during data cleaning/feature extraction as they do not hold much value in enhancing the model's ability to predict information.

- Inverse document frequency
"idf" is used to penalize common occurance terms across different contexts without adding any information. The equation for computing inverse document frequency is:
idf(t)=log1+n1+df(t)+1.
where n  represents the total number of documents,  t  represents the term in question,  df(t)  represents the document frequency of that term (the number of documents within the set of documents containing that term for common terms such as "is", "are", etc.).  idf(t)  will most likely be 1, since all documents are highly likely to contain them (df(t)=n). The less a term occurs across different documents, the smaller the denominator will be, making the fraction bigger and in turn, idf(t) is also bigger.

- Tf-idf
Finally, tf-idf is the product of term-frequency and inverse document frequency, mathematically computed as:
tf−idf(t,d)=tf(t,d)∗idf(t). 
Where  d  represents a document. The more commonly the word appears, the greater the value of tf will be, but if this is the case across different documents, it will be penalized with a small idf. On the other hand, a rarely-occurring word might have a smaller value of tf, but be highlighted by bigger idf values for not occurring often in different documents.

Let's now initialize a TfidfVectorizer object that takes an input of a set of document strings and outputs of the normalized tf-idf vectors. By using fit_transform, we can fit the vectorizer to data and tranform them. Also, there is an option to use the max_df to indicate the cut-off document-frequency for stop words. We shall also set the cut-off document-frequency to be 0.7, which is the lowest possible value that the parameter can take. The final output of fitting & transforming data will give a sparse matrix with the size of n_samples by n_features (number of documents by number of unique words).
"""

my_tfidf = TfidfVectorizer(stop_words='english', max_df=0.7)

# fit the vectorizer and transform X_train into a tf-idf matrix then use the same vectorizer to transform X_test
tfidf_train = my_tfidf.fit_transform(X_train)
tfidf_test = my_tfidf.transform(X_test)

tfidf_train

"""We could now see that there are as many rows as the number of documents and we have extracted over a hundred thousand features or tokens.

### Model training

The model used here is Passive-Aggressive (PA) Classifier, an algorithm that only updates its weights ("aggressive" action) when it encounters examples for which its predictions are wrong else remains unchanged ("passive" action).
Instantiate the PassiveAggressiveClassifier and train it with our features.
"""

from sklearn.linear_model import PassiveAggressiveClassifier

pa_clf = PassiveAggressiveClassifier(max_iter=50)
pa_clf.fit(tfidf_train, y_train)

from mlxtend.plotting import plot_confusion_matrix
y_pred = pa_clf.predict(tfidf_test)

conf_mat = confusion_matrix(y_test, y_pred)
plot_confusion_matrix(conf_mat,
                      show_normed=True, colorbar=True)

accscore = accuracy_score(y_test, y_pred)
f1score = f1_score(y_test,y_pred,pos_label='real')

print('The accuracy of prediction is {:.2f}%.\n'.format(accscore*100))
print('The F1 score is {:.3f}.\n'.format(f1score))

!pip install mlxtend

print("Dimensionality (i.e., number of features): {:d}".format(pa_clf.coef_.shape[1]))
print("Density (i.e., fraction of non-zero elements): {:.3f}".format(density(pa_clf.coef_)))

weights_nonzero = pa_clf.coef_[pa_clf.coef_!=0]
feature_sorter_nonzero = np.argsort(weights_nonzero)
weights_nonzero_sorted =weights_nonzero[feature_sorter_nonzero]

# Plot
fig, axs = plt.subplots(1,2, figsize=(9,3))

sns.lineplot(data=weights_nonzero_sorted, ax=axs[0])
axs[0].set_ylabel('Weight')
axs[0].set_xlabel('Feature number \n (Zero-weight omitted)')

axs[1].hist(weights_nonzero_sorted,
            orientation='horizontal', bins=500,)
axs[1].set_xlabel('Count')

fig.suptitle('Weight distribution in features with non-zero weights')

plt.show()

# Extracting "Indicator" Tokens and Sort features by their associated weights
tokens = my_tfidf.get_feature_names()
tokens_nonzero = np.array(tokens)[pa_clf.coef_[0]!=0]
tokens_nonzero_sorted = np.array(tokens_nonzero)[feature_sorter_nonzero]

num_tokens = 10
fake_indicator_tokens = tokens_nonzero_sorted[:num_tokens]
real_indicator_tokens = np.flip(tokens_nonzero_sorted[-num_tokens:])

fake_indicator = pd.DataFrame({
    'Token': fake_indicator_tokens,
    'Weight': weights_nonzero_sorted[:num_tokens]
})

real_indicator = pd.DataFrame({
    'Token': real_indicator_tokens,
    'Weight': np.flip(weights_nonzero_sorted[-num_tokens:])
})

print('The top {} tokens likely to appear in fake news were the following: \n'.format(num_tokens))
display(fake_indicator)

print('\n\n...and the top {} tokens likely to appear in real news were the following: \n'.format(num_tokens))
display(real_indicator)

fake_contain_fake = fake.text.loc[[np.any([token in body for token in fake_indicator.Token])
                                for body in fake.text.str.lower()]]
real_contain_real = true.text.loc[[np.any([token in body for token in real_indicator.Token])
                                for body in true.text.str.lower()]]

print('Articles that contained any of the matching indicator tokens:\n')

print('FAKE: {} out of {} ({:.2f}%)'
      .format(len(fake_contain_fake), len(fake), len(fake_contain_fake)/len(fake) * 100))
print(fake_contain_fake)

print('\nREAL: {} out of {} ({:.2f}%)'
      .format(len(real_contain_real), len(true), len(real_contain_real)/len(true) * 100))
print(real_contain_real)

# Algorithm "Generalizability"
def FakeNewsDetection(X, y):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)
    
    # vectorizer
    my_tfidf = TfidfVectorizer(stop_words='english', max_df=0.7)
    tfidf_train = my_tfidf.fit_transform(X_train)
    tfidf_test = my_tfidf.transform(X_test)
    
    # model
    my_pac = PassiveAggressiveClassifier(max_iter=50)
    my_pac.fit(tfidf_train, y_train)
    y_pred = my_pac.predict(tfidf_test)
    
    # metrics
    conf_mat = confusion_matrix(y_test, y_pred)
    plot_confusion_matrix(conf_mat,
                          show_normed=True, colorbar=True)
    
    accscore = accuracy_score(y_test, y_pred)
    f1score = f1_score(y_test,y_pred,pos_label='real')

    print('The accuracy of prediction is {:.2f}%.\n'.format(accscore*100))
    print('The F1 score is {:.3f}.\n'.format(f1score))

# Sort non-zero weights
    weights_nonzero = my_pac.coef_[my_pac.coef_!=0]
    feature_sorter_nonzero = np.argsort(weights_nonzero)
    weights_nonzero_sorted =weights_nonzero[feature_sorter_nonzero]
    
# Sort features by their associated weights
    tokens = my_tfidf.get_feature_names()
    tokens_nonzero = np.array(tokens)[my_pac.coef_[0]!=0]
    tokens_nonzero_sorted = np.array(tokens_nonzero)[feature_sorter_nonzero]

    num_tokens = 10
    fake_indicator_tokens = tokens_nonzero_sorted[:num_tokens]
    real_indicator_tokens = np.flip(tokens_nonzero_sorted[-num_tokens:])

    fake_indicator = pd.DataFrame({
        'Token': fake_indicator_tokens,
        'Weight': weights_nonzero_sorted[:num_tokens]
    })

    real_indicator = pd.DataFrame({
        'Token': real_indicator_tokens,
        'Weight': np.flip(weights_nonzero_sorted[-num_tokens:])
    })

    print('The top {} tokens likely to appear in fake news were the following: \n'.format(num_tokens))
    display(fake_indicator)

    print('\n\n...and the top {} tokens likely to appear in real news were the following: \n'.format(num_tokens))
    display(real_indicator)

# Generate a copy of the "real news" dataset and remove headings f

real_copy = true.copy()
for i,body in true.text.items():
    if '(reuters)' in body.lower():
        idx = body.lower().index('(reuters)') + len('(reuters) - ')
        real_copy.text.iloc[i] = body[idx:]
        
real_copy.head()

# Create new data, and run the algorithm
data2 = pd.concat([fake, real_copy], axis=0)
data2 = data2.sample(frac=1).reset_index(drop=True)
data2.drop('subject', axis=1)

FakeNewsDetection(data2['text'], data2['label'])